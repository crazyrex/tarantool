# Replicaset master promotion

* **Status**: In progress
* **Start date**: 02-03-2018
* **Authors**: Vladislav Shpilevoy @Gerold103 \<v.shpilevoy@tarantool.org\>,
Konstantin Osipov @kostja \<kostja@tarantool.org\>
* **Issues**: [#3055](https://github.com/tarantool/tarantool/issues/3055),
[#2625](https://github.com/tarantool/tarantool/issues/2625)

## Summary

Replicaset master promotion is a procedure of atomic making one slave be a new
master, and an old master be a slave in a full-mesh master-slave replicaset.
Master is a replica in read-write mode. Slave is a replica in read-only mode.

Master promotion has API:
```Lua
--
-- Called on a slave promotes its role to master, demoting an old
-- one to slave. Called on a master returns an error.
-- @param opts Optional settings:
--        * timeout - the time in which a promotion must be
--          finished;
--        * quorum - before an old master demotion its data must
--          be synced with no less than quorum slave count,
--          including the being promoted one.
--
-- @retval true The instance is promoted.
-- @retval nil, error The promotion failed.
--
box.ctl.promote(opts)

--
-- Called on a master makes it be slave. Called on a slave returns
-- an error.
-- @param opts The same options as for box.ctl.promote().
--
-- @retval true The instance is demoted.
-- @retval nil, error The demotion failed.
--
box.ctl.demote(opts)

--
-- Status of the latest finished or the currently working
-- promotion round.
-- @retval Empty table. Promote() was not called since the
--         instance had started, or it had started on another
--         instance, that did not sent a promotion info to the
--         current instance yet.
-- @retval status A table with the format:
--    {
--         round_id = <Promotion ID>,
--         round_uuid = <Promotion round UUID>,
--         initiator_uuid = <UUID of the promotion initiator>,
--         timeout = <Timeout of the promotion round>,
--         quorum = <Requested quorum>,
--         role = <The instance role in the round: old master,
--                 watcher, initiator, undefined>,
--         phase = <The round phase: success, error, in progress>,
--         comment = <A human readable comment about the current
--                    promotion status>,
--         old_master_uuid = <UUID of the old master>,
--    }
--
box.ctl.promote_info()

--
-- Remove info about all promotions from the entire cluster.
--
box.ctl.promote_reset()
```

## Background and motivation

The promote procedure strongly simplifies life of developers since they must not
do all of the promotion steps manually, that in a common case is not a trivial
task, as you will see in the algorithm description in the next section.

The common algorithm, disregarding failures and their processing, consists of
the following steps: 
1. On an old master stop accepting DDL/DML - only DQL;
2. Wait until all master data is received by needed slave count, including the
new master candidate;
3. Make the old master be a slave;
4. Make the slave be a new master;
5. Notify all other slaves, that master is changed.

All of the steps are persisted in WAL, that guarantees, that even after a
promotion participant is restarted, after waking up it will not forgot about
promotion.

Demotion of an old master works the same as the promotion but the round is
stopped after demotion.

## Detailed design

Each cluster member has a special system space to distribute promotion steps
over the cluster via replication channels - `_promotion`. Each record in the
space is a promotion message sent by one of instances.
```Lua
format = {}
-- ID of the promotion round. Each round has an unique identifier
-- of two parts: ID and UUID. ID is used to order rounds by the
-- time of their start. Each new round has an ID > than all the
-- known previous ones. Timestamps can not be used since clocks
-- are not perfectly sinced over network.
format[1] = {'id', 'unsigned'}

-- UUID of the promotion round. UUID is generated by a promotion
-- initiator and allows to protect from an error when promotions
-- are started on different nodes at the same time with the same
-- round IDs. UUIDs are different in them because of different
-- initiators.
format[2] = {'round_uuid', 'string'}

-- The promotion round step. It is a Lamport timestampt to
-- preserve partial order of message.
format[3] = {'step', 'unsigned'}

-- UUID of the sender instance.
format[4] = {'source_uuid', 'string'}

-- Timestamp of the message dispatch time by the sender clock.
-- Just debug attribute, that is persisted.
format[5] = {'ts', 'unsigned'}

-- Type is what the sender wants to get or send. Value depends on
-- type.
format[6] = {'type', 'string'}

-- Depending on the message type, different values are stored.
format[7] = {'value', 'map', is_nullable = true}
--
--            Here the type-value pairs are described.
--
-- 'begin'   - the message sent by a promotion initiator to start
--             a round. Value contains promotion metadata: quorum
--             and timeout. And type of the round: is it promotion
--             of a new master or just demotion of an old one.
--
-- 'status'  - the message sent by all promotion participants. It
--             has several goals: cope with a case when the
--             cluster has no masters; when has multiple; to
--             persist read-only cfg flag for recovery. Value has
--             attributes: is_master and is_promoted. It is
--             possible that an instance is already demoted but
--             still is a master until another instance is
--             promoted.
--
-- 'sync'    - the message sent by a master to sync with slaves.
--             Value is nil.
--
-- 'success' - the message sent in response to 'sync'.
--
-- 'demote'  - the message sent by an old master when it collected
--             quorum of syncs.
--
-- 'error'   - an error, that can be sent by any cluster member.
--             For example, it can be failed sync, or an existing
--             promotion is found, or timeout. Value is the error
--             description.
--
-- 'promote' - the message sent by a newly bootstraped instance
--             that is selected as a replicaset master to persist
--             this fact. And by an initiator if a promotion is
--             used in a read-only cluster.
--
s = box.schema.create_space('_promotion', {format = format})
```
To participate in a promotion a cluster member just writes into `_promotion`
space and waits until the record is replicated. This space is cleared by a
garbage collector from finished promotions - with error or success status. Only
latest promotion is not deleted to be able to restore a role after recovery.

Below the protocol is described. On the image the state machine is showed:
![alt text](https://raw.githubusercontent.com/tarantool/tarantool/192feae59a2a0c8d25527db3818ffaad66d788f6/doc/rfc/3055-box_ctl_promote_img1.svg?sanitize=true)

In the simplest case the being promoted instance is a master already -
immediately finish the promotion with the error and with no persisting that. Now
assume promote() is called on a slave. At first, the initiator broadcasts
`begin` request with the promotion status: quorum and timeout.

Each cluster member, received the `begin`, checks if it already knows about
another active promotions. If does, then responds `error` to the newer promotion
request. Else broadcasts `status` message.

After statuses are collected it is possible that a master was not found. In such
a case, if a quorum is gained, the initiator continues without an old master. In
fact, the round becomes just a forced promotion. But a dreadful situation is
possible - the old master could be alive and even accepting user requests but
for a reason it did not manage to participate in the round. And after a time it
connects back to the new master and to replicas and starts sending them rows. In
such emergency these instances should reject the rows of the old master and
force its rejoin to the replicaset.

Indeed, if a new master is elected, it is unacceptable to still consume rows
from an old master. It is a split-brain situation.

Assume that a currently active master was found in the quorum. But it is already
demoted. Then the initiator collects quorum of statuses and does self promotion.

Lets guess an old master is not demoted. It gets `begin` request and enters
read-only mode and broadcasts `sync` request. A slave got `sync` finishes its
participation in the round responding `success`. The old master collects quorum
`success`es. Once the old master has collected responses it broadcasts `demote`.
The initiator, got `demote` from the master, broadcasts `promote` and enters
read-write mode becoming a new master. When an old master gets `promote` it
forgets about its mastery. It means that in next `status` messages it will
send `{is_master = false}` instead of `{is_master = true, is_promoted = false}`.

A special case exists besides: when a cluster is just bootstraped and has no any
concrete master. Via vote messages a replicaset master is elected. But it can
not call box.ctl.promote() to persist that fact because the cluster has no an
old master even in a history. So it broadcasts message `promote` without other
steps of the promotion.

Explicit demote works the same as promote, but no new master is elected. The
round just stops after `demote`.

### Recovery

Recovery is quite simple and merely replays all promotion messages from the
`_promotion` space. But it also does a tricky thing under the hood - it recovers
global `is_master` flag. Consider how is it possible.

During a promotion round several cases exist which persist `is_master` one way
or another.
1. When an instance sends `status` message, its `is_master` is stored in the
message's value as `is_master`.
2. When an instance sends `begin` message to start a promotion, it is not a
master - indeed, only non-master can start a promotion.
3. When an instance sends `begin` message to start a demotion, it is a master.
4. Due to messages reordering from different replication sources it is possible,
that a non-initiator instance has received `sync` message before it succeeded to
send `status`. Then the instance is a watcher and has `is_master = false`.

On recovery the `is_master` value has to be recovered to exactly the same value,
that was before restart, because other instances already aware of this value.
And it can not be changed manually until the promotion history is cleaned up.

## Rationale and alternatives

The protocol has several disputable details.

A one could notice that an old master on `begin` sends two messages: `status`
and `sync` with no an intermediate message. And that `begin` plays both to
notify about the new promotion round and to trigger an old master sync. This
slightly complicates step numbers calculation and a round result figuring out
from the history records. But it reduces number of messages and message types
and optimizes the most common case - regular master promotion in a master-slave
full-mesh cluster.

An alternative - add a new phase between `status` collecting and `sync` of the
old master. It could make the promotion protocol a bit more simple in an
implementation and understanding but on the contrary it is obviously longer.
